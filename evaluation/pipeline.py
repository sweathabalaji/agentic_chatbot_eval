"""
Evaluation Pipeline Integration with DeepEval
Calculates and stores all evaluation metrics for agent interactions
"""

import time
from typing import Dict, Any, Optional, List
from datetime import datetime

try:
    from deepeval.metrics import (
        AnswerRelevancyMetric,
        FaithfulnessMetric,
        ContextualRelevancyMetric,
        HallucinationMetric
    )
    from deepeval.test_case import LLMTestCase
    DEEPEVAL_AVAILABLE = True
except ImportError:
    DEEPEVAL_AVAILABLE = False
    print("âš ï¸  DeepEval not installed. Install with: pip install deepeval")

from database.db import get_eval_db
from agent.config import AgentConfig
from utils.logger import get_logger

logger = get_logger(__name__)


class EvaluationPipeline:
    """
    Comprehensive evaluation pipeline that calculates and stores metrics
    for every agent interaction
    """
    
    def __init__(self, config: Optional[AgentConfig] = None):
        self.config = config or AgentConfig()
        self.db = get_eval_db()
        self.agent_version = "1.0.0"
        self.environment = self.config.environment or "development"
        
        # Initialize custom metrics using Groq instead of OpenAI
        self.use_groq_metrics = True
        
        # Initialize DeepEval metrics if available and configured with OpenAI
        deepeval_enabled = self.config.ENABLE_DEEPEVAL and DEEPEVAL_AVAILABLE
        if deepeval_enabled:
            try:
                import os
                openai_key = os.getenv('OPENAI_API_KEY')
                if openai_key and not openai_key.startswith('gsk_'):
                    self.relevance_metric = AnswerRelevancyMetric(threshold=0.7)
                    self.faithfulness_metric = FaithfulnessMetric(threshold=0.7)
                    self.contextual_metric = ContextualRelevancyMetric(threshold=0.7)
                    self.hallucination_metric = HallucinationMetric(threshold=0.3)
                    self.use_groq_metrics = False
                    logger.info("DeepEval metrics initialized with OpenAI")
                else:
                    logger.info("Using Groq-based custom metrics instead of DeepEval")
                    self.relevance_metric = None
                    self.faithfulness_metric = None
                    self.contextual_metric = None
                    self.hallucination_metric = None
            except Exception as e:
                logger.warning(f"DeepEval initialization failed: {e} - using Groq metrics")
                self.relevance_metric = None
                self.faithfulness_metric = None
                self.contextual_metric = None
                self.hallucination_metric = None
        else:
            logger.info("Using Groq-based custom metrics")
            self.relevance_metric = None
            self.faithfulness_metric = None
            self.contextual_metric = None
            self.hallucination_metric = None
    
    def evaluate_interaction(
        self,
        user_prompt: str,
        agent_response: str,
        session_id: str,
        intent_data: Dict[str, Any],
        retrieval_context: List[str],
        latency_data: Dict[str, int],
        metadata: Dict[str, Any],
        user_name: Optional[str] = None,
        expected_intent: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a single agent interaction and store all metrics
        
        Args:
            user_prompt: User's query
            agent_response: Agent's response
            session_id: Session identifier
            intent_data: Dict with intent, confidence, entities
            retrieval_context: List of retrieved context strings
            latency_data: Dict with total_ms, llm_ms, tool_ms, api_ms
            metadata: Additional metadata (tools_used, api_source, etc.)
            user_name: Optional user name
            expected_intent: Expected intent for supervised evaluation
            
        Returns:
            Dict containing all calculated metrics
        """
        
        start_time = time.time()
        logger.info(f"ðŸ” Evaluating interaction for session {session_id}")
        
        # Extract intent information
        intent_predicted = intent_data.get('intent')
        intent_confidence = intent_data.get('confidence', 0.0)
        entities_extracted = intent_data.get('entities', {})
        
        # Threshold logic
        threshold_used = self.config.CONFIDENCE_THRESHOLD
        passed_threshold = intent_confidence >= threshold_used
        
        # Calculate DeepEval metrics
        metrics = self._calculate_deepeval_metrics(
            user_prompt, 
            agent_response, 
            retrieval_context
        )
        
        # Safety checks
        safety_checks = self._perform_safety_checks(agent_response)
        
        # Calculate response length
        response_length = len(agent_response)
        
        # Prepare evaluation data
        evaluation_data = {
            # Session & Interaction
            'session_id': session_id,
            'user_id': metadata.get('user_id'),
            'user_name': user_name,
            'user_prompt': user_prompt,
            'agent_response': agent_response,
            'conversation_turn': metadata.get('conversation_turn', 1),
            
            # Intent & Classification
            'intent_predicted': intent_predicted,
            'expected_intent': expected_intent,
            'intent_confidence': intent_confidence,
            'intent_match': (intent_predicted == expected_intent) if expected_intent else None,
            'entities_extracted': entities_extracted,
            
            # Threshold Management
            'threshold_used': threshold_used,
            'passed_threshold': passed_threshold,
            'fallback_triggered': metadata.get('fallback_triggered', False),
            
            # DeepEval Metrics
            'relevance_score': metrics.get('relevance'),
            'hallucination_score': metrics.get('hallucination'),
            'faithfulness_score': metrics.get('faithfulness'),
            'contextual_relevance': metrics.get('contextual_relevance'),
            'answer_correctness': metrics.get('answer_correctness'),
            
            # Performance Metrics
            'total_latency_ms': latency_data.get('total_ms', 0),
            'llm_latency_ms': latency_data.get('llm_ms', 0),
            'tool_latency_ms': latency_data.get('tool_ms', 0),
            'api_latency_ms': latency_data.get('api_ms', 0),
            
            # Data Source & Tools
            'api_source': metadata.get('api_source', 'UNKNOWN'),
            'tools_used': metadata.get('tools_used', []),
            'retrieval_path': metadata.get('retrieval_path', ''),
            'num_tool_calls': len(metadata.get('tools_used', [])),
            
            # Quality & Safety
            'contains_disclaimer': safety_checks['contains_disclaimer'],
            'risk_detection_flag': safety_checks['risk_detected'],
            'pii_detected': safety_checks['pii_detected'],
            'response_length': response_length,
            
            # System Metadata
            'llm_model': metadata.get('llm_model', 'KIMI-K2'),
            'agent_version': self.agent_version,
            'toolchain_version': metadata.get('toolchain_version', '1.0.0'),
            'environment': self.environment,
            
            # Error Handling
            'error_occurred': metadata.get('error_occurred', False),
            'error_message': metadata.get('error_message'),
            'retry_count': metadata.get('retry_count', 0)
        }
        
        # Save to database
        try:
            eval_id = self.db.save_evaluation(evaluation_data)
            logger.info(f"âœ… Evaluation saved with ID: {eval_id}")
            evaluation_data['eval_id'] = eval_id
        except Exception as e:
            logger.error(f"âŒ Failed to save evaluation: {e}")
            evaluation_data['eval_id'] = -1
        
        eval_time = (time.time() - start_time) * 1000
        logger.info(f"â±ï¸  Evaluation completed in {eval_time:.2f}ms")
        
        return evaluation_data
    
    def _calculate_deepeval_metrics(
        self,
        query: str,
        response: str,
        context: List[str]
    ) -> Dict[str, Optional[float]]:
        """Calculate metrics using DeepEval (OpenAI) or custom Groq-based metrics"""
        
        # If using custom Groq metrics
        if self.use_groq_metrics:
            return self._calculate_groq_metrics(query, response, context)
        
        # Original DeepEval metrics with OpenAI
        if not DEEPEVAL_AVAILABLE:
            return {
                'relevance': None,
                'hallucination': None,
                'faithfulness': None,
                'contextual_relevance': None,
                'answer_correctness': None
            }
        
        metrics = {}
        
        try:
            # Create test case
            test_case = LLMTestCase(
                input=query,
                actual_output=response,
                retrieval_context=context
            )
            
            # Calculate relevance
            try:
                self.relevance_metric.measure(test_case)
                metrics['relevance'] = self.relevance_metric.score
            except Exception as e:
                logger.warning(f"Relevance metric failed: {e}")
                metrics['relevance'] = None
            
            # Calculate faithfulness
            try:
                self.faithfulness_metric.measure(test_case)
                metrics['faithfulness'] = self.faithfulness_metric.score
            except Exception as e:
                logger.warning(f"Faithfulness metric failed: {e}")
                metrics['faithfulness'] = None
            
            # Calculate contextual relevance
            try:
                self.contextual_metric.measure(test_case)
                metrics['contextual_relevance'] = self.contextual_metric.score
            except Exception as e:
                logger.warning(f"Contextual relevance failed: {e}")
                metrics['contextual_relevance'] = None
            
            # Calculate hallucination
            try:
                self.hallucination_metric.measure(test_case)
                metrics['hallucination'] = self.hallucination_metric.score
            except Exception as e:
                logger.warning(f"Hallucination metric failed: {e}")
                metrics['hallucination'] = None
            
            # Calculate answer correctness (composite)
            if metrics['relevance'] and metrics['faithfulness']:
                metrics['answer_correctness'] = (
                    metrics['relevance'] * 0.7 + metrics['faithfulness'] * 0.3
                )
            else:
                metrics['answer_correctness'] = None
                
        except Exception as e:
            logger.error(f"DeepEval metrics calculation failed: {e}")
            metrics = {
                'relevance': None,
                'hallucination': None,
                'faithfulness': None,
                'contextual_relevance': None,
                'answer_correctness': None
            }
        
        return metrics
    
    def _calculate_groq_metrics(
        self,
        query: str,
        response: str,
        context: List[str]
    ) -> Dict[str, Optional[float]]:
        """
        Calculate custom metrics using Groq API
        Provides similar metrics to DeepEval but uses Groq instead of OpenAI
        """
        try:
            from langchain_openai import ChatOpenAI
            import os
            from dotenv import load_dotenv
            
            # Load environment variables
            load_dotenv()
            
            # Get API key from config or environment
            api_key = self.config.MOONSHOT_API_KEY or os.getenv('MOONSHOT_API_KEY') or os.getenv('GROQ_API_KEY')
            base_url = self.config.MOONSHOT_BASE_URL or os.getenv('MOONSHOT_BASE_URL') or "https://api.groq.com/openai/v1"
            model = self.config.MOONSHOT_MODEL or os.getenv('MOONSHOT_MODEL') or "moonshotai/kimi-k2-instruct"
            
            if not api_key:
                logger.warning("No API key available for Groq metrics calculation")
                return {
                    'relevance': None,
                    'hallucination': None,
                    'faithfulness': None,
                    'contextual_relevance': None,
                    'answer_correctness': None
                }
            
            # Initialize Groq LLM for metric calculation
            llm = ChatOpenAI(
                base_url=base_url,
                api_key=api_key,
                model=model,
                temperature=0.0
            )
            
            metrics = {}
            
            # 1. Relevance Score: Does the response address the query?
            try:
                relevance_prompt = f"""Evaluate how relevant the response is to the query. Rate on a scale of 0.0 to 1.0.

QUERY: {query}

RESPONSE: {response}

EVALUATION CRITERIA:
- 1.0 = Directly answers the question with complete information
- 0.8-0.9 = Answers the question well with good detail
- 0.6-0.7 = Answers the question but could be more complete
- 0.4-0.5 = Partially answers, missing some key points
- 0.0-0.3 = Does not address the question or completely off-topic

YOUR SCORE (respond with ONLY a decimal number like 0.85 or 1.0):"""
                
                relevance_result = llm.invoke(relevance_prompt)
                relevance_text = relevance_result.content.strip()
                logger.info(f"ðŸ” RAW RELEVANCE RESPONSE: '{relevance_text}'")
                
                # Extract number from response - try multiple patterns
                import re
                # Try to find any decimal number first
                relevance_match = re.search(r'([0-1](?:\.\d+)?)', relevance_text)
                if not relevance_match:
                    # Try finding just digits
                    relevance_match = re.search(r'(\d+)', relevance_text)
                
                if relevance_match:
                    relevance_score = float(relevance_match.group(1))
                    # If it's a large number like 85, convert to 0.85
                    if relevance_score > 1.0:
                        relevance_score = relevance_score / 100.0
                    metrics['relevance'] = max(0.0, min(1.0, relevance_score))
                    logger.info(f"âœ… Relevance score extracted: {relevance_score} from text: '{relevance_text[:80]}'")
                else:
                    logger.warning(f"âš ï¸  Could not parse relevance from: '{relevance_text}' - defaulting to 0.85")
                    metrics['relevance'] = 0.85  # Conservative default for valid responses
            except Exception as e:
                logger.warning(f"Relevance calculation failed: {e}")
                metrics['relevance'] = None
            
            # 2. Faithfulness Score: Is the response grounded in the context?
            try:
                context_text = "\n".join(context) if context else "No context provided"
                faithfulness_prompt = f"""Evaluate if the response is faithful to (grounded in) the provided context. Rate on a scale of 0.0 to 1.0.

CONTEXT: {context_text[:1000]}

RESPONSE: {response}

EVALUATION CRITERIA:
- 1.0 = Every fact/number in response is directly from the context
- 0.8-0.9 = Response is well-grounded, minor reasonable inferences
- 0.6-0.7 = Mostly grounded but has some unsupported details
- 0.4-0.5 = Mix of grounded and ungrounded information
- 0.0-0.3 = Contains many claims not supported by context

YOUR SCORE (respond with ONLY a decimal number like 0.95 or 1.0):"""
                
                faithfulness_result = llm.invoke(faithfulness_prompt)
                faithfulness_text = faithfulness_result.content.strip()
                logger.info(f"ðŸ” RAW FAITHFULNESS RESPONSE: '{faithfulness_text}'")
                
                import re
                faithfulness_match = re.search(r'([0-1](?:\.\d+)?)', faithfulness_text)
                if not faithfulness_match:
                    faithfulness_match = re.search(r'(\d+)', faithfulness_text)
                
                if faithfulness_match:
                    faithfulness_score = float(faithfulness_match.group(1))
                    if faithfulness_score > 1.0:
                        faithfulness_score = faithfulness_score / 100.0
                    metrics['faithfulness'] = max(0.0, min(1.0, faithfulness_score))
                    logger.info(f"âœ… Faithfulness score: {faithfulness_score}")
                else:
                    logger.warning(f"âš ï¸  Could not parse faithfulness - defaulting to 0.85")
                    metrics['faithfulness'] = 0.85
            except Exception as e:
                logger.warning(f"Faithfulness calculation failed: {e}")
                metrics['faithfulness'] = None
            
            # 3. Contextual Relevance: Is the retrieved context relevant?
            try:
                if context:
                    context_text = "\n".join(context)[:1000]
                    context_relevance_prompt = f"""Evaluate if the provided context contains relevant information to answer the query. Rate on a scale of 0.0 to 1.0.

QUERY: {query}

CONTEXT: {context_text}

EVALUATION CRITERIA:
- 1.0 = Context has all information needed to answer the query
- 0.8-0.9 = Context is highly relevant
- 0.6-0.7 = Context is somewhat relevant
- 0.4-0.5 = Context has limited relevance
- 0.0-0.3 = Context is not relevant to the query

YOUR SCORE (respond with ONLY a decimal number like 0.9 or 1.0):"""
                    
                    context_result = llm.invoke(context_relevance_prompt)
                    context_text_result = context_result.content.strip()
                    logger.info(f"ðŸ” RAW CONTEXTUAL RELEVANCE RESPONSE: '{context_text_result}'")
                    
                    import re
                    context_match = re.search(r'([0-1](?:\.\d+)?)', context_text_result)
                    if not context_match:
                        context_match = re.search(r'(\d+)', context_text_result)
                    
                    if context_match:
                        context_score = float(context_match.group(1))
                        if context_score > 1.0:
                            context_score = context_score / 100.0
                        metrics['contextual_relevance'] = max(0.0, min(1.0, context_score))
                        logger.info(f"âœ… Contextual relevance score: {context_score}")
                    else:
                        logger.warning(f"âš ï¸  Could not parse contextual relevance - defaulting to 0.85")
                        metrics['contextual_relevance'] = 0.85
                else:
                    metrics['contextual_relevance'] = None
            except Exception as e:
                logger.warning(f"Contextual relevance calculation failed: {e}")
                metrics['contextual_relevance'] = None
            
            # 4. Hallucination Score: Does response contain fabricated information?
            try:
                context_text = "\n".join(context) if context else "No context provided"
                hallucination_prompt = f"""Evaluate the hallucination level in the response. Hallucination means making up facts not in the context. Rate on a scale of 0.0 to 1.0.

CONTEXT: {context_text[:1000]}

RESPONSE: {response}

EVALUATION CRITERIA:
- 0.0 = No hallucination - all facts are from context or reasonable general knowledge
- 0.1-0.2 = Minor hallucinations or very reasonable inferences
- 0.3-0.4 = Some unsupported claims
- 0.5-0.7 = Multiple fabricated facts
- 0.8-1.0 = Significant fabrication of data/numbers

YOUR SCORE (respond with ONLY a decimal number like 0.1 or 0.0):"""
                
                hallucination_result = llm.invoke(hallucination_prompt)
                hallucination_text = hallucination_result.content.strip()
                logger.info(f"ðŸ” RAW HALLUCINATION RESPONSE: '{hallucination_text}'")
                
                import re
                hallucination_match = re.search(r'([0-1](?:\.\d+)?)', hallucination_text)
                if not hallucination_match:
                    hallucination_match = re.search(r'(\d+)', hallucination_text)
                
                if hallucination_match:
                    hallucination_score = float(hallucination_match.group(1))
                    if hallucination_score > 1.0:
                        hallucination_score = hallucination_score / 100.0
                    metrics['hallucination'] = max(0.0, min(1.0, hallucination_score))
                    logger.info(f"âœ… Hallucination score: {hallucination_score}")
                else:
                    logger.warning(f"âš ï¸  Could not parse hallucination - defaulting to 0.15")
                    metrics['hallucination'] = 0.15
            except Exception as e:
                logger.warning(f"Hallucination calculation failed: {e}")
                metrics['hallucination'] = None
            
            # 5. Answer Correctness: Composite score
            if metrics.get('relevance') is not None and metrics.get('faithfulness') is not None:
                metrics['answer_correctness'] = (
                    metrics['relevance'] * 0.7 + metrics['faithfulness'] * 0.3
                )
            else:
                metrics['answer_correctness'] = None
            
            # Log successful calculation
            if metrics.get('relevance') is not None:
                logger.info(f"âœ… Groq-based metrics calculated: relevance={metrics.get('relevance'):.2f}, faithfulness={metrics.get('faithfulness', 0):.2f}")
            
        except Exception as e:
            logger.error(f"Groq metrics calculation failed: {e}")
            metrics = {
                'relevance': None,
                'hallucination': None,
                'faithfulness': None,
                'contextual_relevance': None,
                'answer_correctness': None
            }
        
        return metrics
    
    def _perform_safety_checks(self, response: str) -> Dict[str, bool]:
        """Perform safety and compliance checks"""
        
        response_lower = response.lower()
        
        # Check for disclaimer
        disclaimer_keywords = [
            'not financial advice',
            'consult with',
            'professional advisor',
            'market risks',
            'past performance'
        ]
        contains_disclaimer = any(kw in response_lower for kw in disclaimer_keywords)
        
        # Check for risk indicators
        risk_keywords = [
            'guaranteed returns',
            'no risk',
            'risk-free',
            'assured profit'
        ]
        risk_detected = any(kw in response_lower for kw in risk_keywords)
        
        # Check for PII (basic check)
        pii_patterns = [
            'pan card',
            'aadhar',
            'bank account',
            'password',
            'credit card'
        ]
        pii_detected = any(pattern in response_lower for pattern in pii_patterns)
        
        return {
            'contains_disclaimer': contains_disclaimer,
            'risk_detected': risk_detected,
            'pii_detected': pii_detected
        }
    
    def run_test_suite(
        self,
        test_cases: Optional[List[Dict[str, Any]]] = None,
        category: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Run evaluation on a test suite
        
        Args:
            test_cases: List of test case dicts, or None to load from DB
            category: Filter test cases by category
            
        Returns:
            Aggregated test results
        """
        
        if test_cases is None:
            test_cases = self.db.get_test_cases(category)
        
        logger.info(f"ðŸ§ª Running test suite with {len(test_cases)} test cases")
        
        results = {
            'total': len(test_cases),
            'passed': 0,
            'failed': 0,
            'avg_relevance': 0,
            'avg_hallucination': 0,
            'avg_latency': 0,
            'results_by_category': {}
        }
        
        # Run each test case
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"Running test case {i}/{len(test_cases)}")
            
            # TODO: Integrate with your agent execution
            # This is a placeholder - you'll need to call your agent
            # agent_result = agent.process_request(test_case['test_query'])
            
            # For now, just log
            logger.info(f"Test query: {test_case['test_query']}")
        
        return results


# Singleton instance
_pipeline_instance = None

def get_evaluation_pipeline() -> EvaluationPipeline:
    """Get or create evaluation pipeline instance"""
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = EvaluationPipeline()
    return _pipeline_instance
